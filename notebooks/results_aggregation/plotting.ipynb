{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "dir2 = os.path.abspath(\"\")\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path:\n",
    "    sys.path.append(dir1)\n",
    "os.chdir(\"../..\")\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import mean\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.utils.aggregation import filter_data, aggregate_methods\n",
    "from src.compare_methods import run_CD, run_dolan_more, bayes_scores\n",
    "from src.compare_methods.votenrank import run_printtable_votenrank\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_RES = os.path.join(\"results\", \"aggregation\")\n",
    "new_res = pd.read_csv(os.path.join(PATH_TO_RES, \"metrics_ndcg_10.csv\"), index_col=0)\n",
    "\n",
    "# k = 10\n",
    "# metric = \"ndcg\"\n",
    "\n",
    "# metrics_df = new_res[(new_res[\"Metric\"] == metric) & (new_res[\"k\"] == k)].copy()\n",
    "# metrics_df.drop(columns=[\"Metric\", \"k\"], inplace=True)\n",
    "metrics_df = new_res.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "fontsize = 14\n",
    "\n",
    "color_map = {\n",
    "    0: \"#1f77b4\",\n",
    "    1: \"#ff7f0e\",\n",
    "    2: \"#2ca02c\",\n",
    "    3: \"#d62728\",\n",
    "    4: \"#9467bd\",\n",
    "    5: \"#8c564b\",\n",
    "    6: \"#e377c2\",\n",
    "    7: \"#7f7f7f\",\n",
    "    8: \"#bcbd22\",\n",
    "    9: \"#17becf\",\n",
    "}\n",
    "name_map = {\n",
    "    \"dm_value\": \"DM AUC\",\n",
    "    \"cd_score\": \"Mean ranks\",\n",
    "    \"gm_value\": \"Geom. mean\",\n",
    "    \"mr_value\": \"MA\",\n",
    "    \"hm_value\": \"Harm. mean\",\n",
    "    \"Copeland_value\": \"Copeland\",\n",
    "    \"Minimax_value\": \"Minimax\",\n",
    "    \"dm_lbo_value\": \"DM LBO\",\n",
    "}\n",
    "name2idx = {\n",
    "    \"dm_value\": 0,\n",
    "    \"cd_score\": 1,\n",
    "    \"gm_value\": 2,\n",
    "    \"mr_value\": 3,\n",
    "    \"hm_value\": 4,\n",
    "    \"Copeland_value\": 5,\n",
    "    \"Minimax_value\": 6,\n",
    "    \"dm_lbo_value\": 7,\n",
    "}\n",
    "name2marker = {\n",
    "    \"dm_value\": 0,\n",
    "    \"cd_score\": 1,\n",
    "    \"gm_value\": 2,\n",
    "    \"mr_value\": 3,\n",
    "    \"hm_value\": 4,\n",
    "    \"Copeland_value\": 5,\n",
    "    \"Minimax_value\": 6,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "\n",
    "def calculate_metrics(data, votenrank_list=None, drop_cd: bool = False) -> pd.DataFrame:\n",
    "    results_DM = run_dolan_more(data, save_image=False)\n",
    "    results_DM.rename(columns={\"score\": \"dm_score\", \"ranks\": \"dm_value\"}, inplace=True)\n",
    "    if not drop_cd:\n",
    "        results_CD = run_CD(data)\n",
    "        results_CD.rename(\n",
    "            columns={\"score\": \"cd_score\", \"ranks\": \"cd_ranks\"}, inplace=True\n",
    "        )\n",
    "        q = pd.merge(results_DM, results_CD, on=[\"Model_name\"])\n",
    "    else:\n",
    "        q = results_DM\n",
    "\n",
    "    results_new_DM = run_dolan_more(data, save_image=False, mode=\"leave_best_out\")\n",
    "    # results_new_DM.rename(columns={\"score\": \"dm_lbo_score\", \"ranks\": \"dm_lbo_value\"}, inplace=True)\n",
    "    q[\"dm_lbo_value\"] = q[\"Model_name\"].map(results_new_DM)\n",
    "\n",
    "    # q = pd.merge(q, results_new_DM, on=['Model_name'])\n",
    "\n",
    "    q.set_index(\"Model_name\", inplace=True)\n",
    "    metrics_MR = data.copy()\n",
    "\n",
    "    metrics_MR[\"gm_score\"] = metrics_MR.groupby(\"Method\")[\"Value\"].transform(\n",
    "        stats.gmean\n",
    "    )\n",
    "    metrics_MR[\"mr_score\"] = metrics_MR.groupby(\"Method\")[\"Value\"].transform(mean)\n",
    "    metrics_MR[\"hm_score\"] = metrics_MR.groupby(\"Method\")[\"Value\"].transform(\n",
    "        stats.hmean\n",
    "    )\n",
    "\n",
    "    metrics_MR = metrics_MR.drop(columns=[\"Dataset\", \"Value\"]).drop_duplicates()\n",
    "\n",
    "    metrics_MR[\"gm_value\"] = metrics_MR[\"gm_score\"].rank(\n",
    "        method=\"average\", ascending=False\n",
    "    )\n",
    "    metrics_MR[\"mr_value\"] = metrics_MR[\"mr_score\"].rank(\n",
    "        method=\"average\", ascending=False\n",
    "    )\n",
    "    metrics_MR[\"hm_value\"] = metrics_MR[\"hm_score\"].rank(\n",
    "        method=\"average\", ascending=False\n",
    "    )\n",
    "    metrics_MR.set_index(\"Method\", inplace=True)\n",
    "    q = pd.merge(q, metrics_MR, right_index=True, left_index=True)\n",
    "\n",
    "    if votenrank_list is not None:\n",
    "        metrics_votenrank = run_printtable_votenrank(data)\n",
    "        for x in votenrank_list:\n",
    "            metrics_method = (\n",
    "                metrics_votenrank[x]\n",
    "                .reset_index()\n",
    "                .rename(columns={\"Ranking position\": f\"{x}_value\"})\n",
    "            )\n",
    "            metrics_method[\"Model_name\"] = metrics_method.apply(\n",
    "                lambda y: str(y[x]).split(\": \")[1].split(\"\\n\")[0], axis=1\n",
    "            )\n",
    "            metrics_method[x + \"_score\"] = metrics_method.apply(\n",
    "                lambda y: float(str(y[x]).split(\": \")[0]), axis=1\n",
    "            )\n",
    "            metrics_method.set_index(\"Model_name\", inplace=True)\n",
    "            # metrics_method.drop(columns=[x], inplace=True)\n",
    "            q = pd.merge(q, metrics_method, right_index=True, left_index=True)\n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot drop data case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_list = metrics_df[\"Dataset\"].unique()\n",
    "drop_list = [list(combinations(datasets_list, i)) for i in range(1, 4)]\n",
    "drop_list = flatten(drop_list)\n",
    "\n",
    "sampling = 100\n",
    "n_data = metrics_df[\"Dataset\"].nunique()\n",
    "votenrank_list = [\"Copeland\", \"Minimax\"]\n",
    "current_res = calculate_metrics(metrics_df, votenrank_list=votenrank_list)\n",
    "\n",
    "ranks_columns = [x for x in current_res.columns if \"value\" in x or x == \"cd_score\"]\n",
    "name2idx = {x: i for i, x in enumerate(ranks_columns)}\n",
    "\n",
    "# methods_list = list(metrics_df['Method'].unique())\n",
    "res_sp_mean = {x: [] for x in ranks_columns}\n",
    "res_sp_std = {x: [] for x in ranks_columns}\n",
    "\n",
    "sample_size = []\n",
    "\n",
    "# fix seed\n",
    "np.random.seed(0)\n",
    "\n",
    "for i in tqdm(range(1, n_data)):  # len(datasets_list) - 1\n",
    "    sp_df = pd.DataFrame()\n",
    "    drop_list = list(combinations(datasets_list, i))\n",
    "    # print(drop_list)\n",
    "    if len(drop_list) < sampling:\n",
    "        drop_case = drop_list\n",
    "        sample_size.append(len(drop_list))\n",
    "    else:\n",
    "        data_idx = np.random.choice(\n",
    "            np.arange(len(drop_list)), size=sampling, replace=False\n",
    "        )\n",
    "        drop_case = [drop_list[x] for x in data_idx]\n",
    "        sample_size.append(sampling)\n",
    "\n",
    "    for j, drop_group in enumerate(drop_case):\n",
    "        tmp_df = metrics_df[~metrics_df[\"Dataset\"].isin(drop_group)].copy()\n",
    "        output_df = calculate_metrics(tmp_df, votenrank_list=votenrank_list)\n",
    "        # output_df.to_csv(save_path / f\"{'_'.join(drop_group)}.csv\")\n",
    "\n",
    "        for x in ranks_columns:\n",
    "            # ken_df.loc[j, x] = stats.kendalltau(current_res[x], output_df[x])[0]\n",
    "            sp_df.loc[j, x] = stats.spearmanr(current_res[x], output_df[x])[0]\n",
    "    for x in ranks_columns:\n",
    "        res_sp_mean[x] += [sp_df.mean().to_dict()[x]]\n",
    "        res_sp_std[x] += [sp_df.std().to_dict()[x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(7, 5), dpi=200)\n",
    "\n",
    "for r_method in res_sp_mean:\n",
    "    plt.plot(\n",
    "        n_data - np.arange(1, n_data),\n",
    "        res_sp_mean[r_method],\n",
    "        label=name_map[r_method],\n",
    "        # marker='.',\n",
    "        color=color_map[name2idx[r_method]],\n",
    "    )\n",
    "    # some confidence interval\n",
    "    ci = 1.96 * np.array(res_sp_std[r_method]) / np.sqrt(sampling)\n",
    "    plt.fill_between(\n",
    "        n_data - np.arange(1, n_data),\n",
    "        np.array(res_sp_mean[r_method]) - ci,\n",
    "        np.array(res_sp_mean[r_method]) + ci,\n",
    "        color=color_map[name2idx[r_method]],\n",
    "        alpha=0.1,\n",
    "    )\n",
    "\n",
    "plt.legend(\n",
    "    loc=\"lower right\",\n",
    "    fontsize=fontsize - 2,\n",
    ")\n",
    "plt.xticks(\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "plt.yticks(\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "plt.xlabel(\n",
    "    r\"Number of datasets\",\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "plt.ylabel(\n",
    "    r\"Spearman's $\\rho$\",\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "plt.show()\n",
    "plt.savefig(os.path.join(PATH_TO_RES, \"sp_drop_data_with_ci_test.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparam tuning for DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:09<00:00,  4.36it/s]\n"
     ]
    }
   ],
   "source": [
    "beta_list = np.linspace(1.5, 10, 40)\n",
    "\n",
    "dm_corr = []\n",
    "dm_new_corr = []\n",
    "\n",
    "results_DM = run_dolan_more(metrics_df, save_image=False)\n",
    "results_DM_new = run_dolan_more(\n",
    "    metrics_df,\n",
    "    save_image=False,\n",
    "    mode=\"leave_best_out\",\n",
    ")\n",
    "results_DM[\"dm_lbo_value\"] = results_DM[\"Model_name\"].map(results_DM_new)\n",
    "\n",
    "\n",
    "results_DM.set_index(\"Model_name\", inplace=True)\n",
    "\n",
    "for beta in tqdm(beta_list):\n",
    "    results_DM_os = run_dolan_more(\n",
    "        metrics_df,\n",
    "        save_image=False,\n",
    "        max_beta=beta,\n",
    "    )\n",
    "    results_DM_ns = run_dolan_more(\n",
    "        metrics_df,\n",
    "        save_image=False,\n",
    "        max_beta=beta,\n",
    "        mode=\"leave_best_out\",\n",
    "    )\n",
    "    results_DM_os[\"dm_lbo_value\"] = results_DM_os[\"Model_name\"].map(results_DM_ns)\n",
    "    results_DM_os.set_index(\"Model_name\", inplace=True)\n",
    "\n",
    "    dm_corr.append(stats.spearmanr(results_DM_os[\"ranks\"], results_DM[\"ranks\"])[0])\n",
    "    dm_new_corr.append(\n",
    "        stats.spearmanr(results_DM_os[\"dm_lbo_value\"], results_DM[\"dm_lbo_value\"])[0]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "beta_default = 3.0\n",
    "\n",
    "plt.figure(figsize=(7, 6), dpi=200)\n",
    "plt.plot(\n",
    "    beta_list,\n",
    "    dm_corr,\n",
    "    label=name_map[\"dm_value\"],\n",
    "    # marker='.',\n",
    "    color=color_map[name2idx[\"dm_value\"]],\n",
    ")\n",
    "plt.plot(\n",
    "    beta_list,\n",
    "    dm_new_corr,\n",
    "    label=name_map[\"dm_lbo_value\"],\n",
    "    # marker='.',\n",
    "    color=color_map[name2idx[\"dm_lbo_value\"]],\n",
    ")\n",
    "plt.axvline(\n",
    "    x=beta_default,\n",
    "    color=color_map[4],\n",
    "    linestyle=\"--\",\n",
    "    label=r\"Default $\\hat{\\beta}$ DM\",\n",
    ")\n",
    "\n",
    "plt.legend(\n",
    "    loc=\"lower right\",\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "plt.xticks(\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "plt.yticks(\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "plt.xlabel(\n",
    "    r\"$\\hat{\\beta}$\",\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "plt.ylabel(\n",
    "    r\"Spearman's $\\tau$\",\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "plt.show()\n",
    "plt.savefig(os.path.join(PATH_TO_RES, \"test_hp_changing_test.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop methods case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cd = False\n",
    "n_data = 8\n",
    "\n",
    "current_res = calculate_metrics(\n",
    "    metrics_df, votenrank_list=[\"Minimax\", \"Copeland\"], drop_cd=drop_cd\n",
    ")  #\n",
    "current_res.index.names = [\"Model_name\"]\n",
    "current_res.reset_index(inplace=True)\n",
    "ranks_columns = [x for x in current_res.columns if \"value\" in x or x == \"cd_score\"]\n",
    "name2idx = {x: i for i, x in enumerate(ranks_columns)}\n",
    "res_ken_mean = {x: [] for x in ranks_columns}\n",
    "res_ken_std = {x: [] for x in ranks_columns}\n",
    "\n",
    "sampling = []\n",
    "\n",
    "for i in tqdm(range(1, n_data)):\n",
    "    drop_list = combinations(metrics_df[\"Method\"].unique(), i)\n",
    "    tmp_ken_mean = {x: [] for x in ranks_columns}\n",
    "    k = 0\n",
    "    for x in drop_list:\n",
    "        tmp_data = metrics_df[~metrics_df[\"Method\"].isin(x)].copy()\n",
    "        tmp_res = calculate_metrics(\n",
    "            tmp_data, votenrank_list=[\"Minimax\", \"Copeland\"], drop_cd=drop_cd\n",
    "        )\n",
    "        tmp_current_res = current_res[~current_res[\"Model_name\"].isin(x)]\n",
    "        for r_method in ranks_columns:\n",
    "            tmp_ken_mean[r_method].append(\n",
    "                stats.spearmanr(tmp_res[r_method], tmp_current_res[r_method])[0]\n",
    "            )\n",
    "        k += 1\n",
    "    for r_method in ranks_columns:\n",
    "        res_ken_mean[r_method].append(np.mean(tmp_ken_mean[r_method]))\n",
    "        res_ken_std[r_method].append(np.std(tmp_ken_mean[r_method]))\n",
    "    sampling.append(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "\n",
    "plt.figure(figsize=(7, 5), dpi=200)\n",
    "for r_method in res_ken_mean:\n",
    "    plt.plot(\n",
    "        n_data + 2 - np.arange(1, n_data),\n",
    "        res_ken_mean[r_method],\n",
    "        label=name_map[r_method],\n",
    "        # marker='.',\n",
    "        color=color_map[name2idx[r_method]],\n",
    "    )\n",
    "    # some confidence interval\n",
    "    ci = 1.96 * np.array(res_ken_std[r_method]) / np.sqrt(np.array(sampling))\n",
    "    plt.fill_between(\n",
    "        n_data + 2 - np.arange(1, n_data),\n",
    "        np.array(res_ken_mean[r_method]) - ci,\n",
    "        np.array(res_ken_mean[r_method]) + ci,\n",
    "        color=color_map[name2idx[r_method]],\n",
    "        alpha=0.1,\n",
    "    )\n",
    "\n",
    "plt.legend(\n",
    "    loc=\"lower right\",\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "plt.xticks(\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "plt.yticks(\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "plt.xlabel(\n",
    "    r\"Number of methods\",\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "plt.ylabel(\n",
    "    r\"Spearman's $\\rho$\",\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "plt.show()\n",
    "plt.savefig(os.path.join(PATH_TO_RES, \"test_sp_drop_methods_ci_wo_cd_test.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding better method case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increase_column(value, alpha):\n",
    "    return min(value * alpha, 1.0)\n",
    "\n",
    "\n",
    "alpha_space = np.linspace(1.0, 4.0, 40)\n",
    "\n",
    "drop_cd = False  # True\n",
    "\n",
    "current_res = calculate_metrics(\n",
    "    metrics_df, votenrank_list=[\"Minimax\", \"Copeland\"], drop_cd=drop_cd\n",
    ").reset_index()\n",
    "ranks_columns = [x for x in current_res.columns if \"value\" in x or x == \"cd_score\"]\n",
    "name2idx = {x: i for i, x in enumerate(ranks_columns)}\n",
    "res_ken_mean = {x: [] for x in ranks_columns}\n",
    "res_ken_std = {x: [] for x in ranks_columns}\n",
    "\n",
    "sampling = []\n",
    "\n",
    "new_method = \"Method A\"\n",
    "best_res = {\"Value\": dict(metrics_df.groupby(by=\"Dataset\")[\"Value\"].max())}\n",
    "\n",
    "for alpha in tqdm(alpha_space):\n",
    "    best_res_df = pd.DataFrame(best_res)\n",
    "\n",
    "    best_res_df.index.names = [\"Dataset\"]\n",
    "    best_res_df[\"Method\"] = new_method\n",
    "    incr_value = lambda x: increase_column(x, alpha)\n",
    "    best_res_df[\"Value\"] = best_res_df[\"Value\"].apply(incr_value)\n",
    "\n",
    "    best_res_df.reset_index(inplace=True)\n",
    "    tmp_df = pd.concat([metrics_df, best_res_df], ignore_index=True)\n",
    "\n",
    "    tmp_res = calculate_metrics(\n",
    "        tmp_df, votenrank_list=[\"Minimax\", \"Copeland\"], drop_cd=drop_cd\n",
    "    )\n",
    "    tmp_res.drop(index=new_method, inplace=True)\n",
    "\n",
    "    for r_method in ranks_columns:\n",
    "        res_ken_mean[r_method].append(\n",
    "            stats.spearmanr(tmp_res[r_method], current_res[r_method])[0]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(7, 5), dpi=200)\n",
    "\n",
    "for r_method in res_ken_mean:\n",
    "    plt.plot(\n",
    "        alpha_space,\n",
    "        res_ken_mean[r_method],\n",
    "        label=name_map[r_method],\n",
    "        # marker='.',\n",
    "        color=color_map[name2idx[r_method]],\n",
    "    )\n",
    "\n",
    "plt.legend(\n",
    "    loc=\"lower right\",\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "plt.xticks(\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "plt.yticks(\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "plt.xlabel(\n",
    "    r\"$\\alpha$\",\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "plt.ylabel(\n",
    "    r\"Spearman's $\\tau$\",\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "plt.show()\n",
    "plt.savefig(os.path.join(PATH_TO_RES, \"add_better_method_case_test.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add new similar method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increase_column(value, alpha):\n",
    "    return min(value * alpha, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [07:39<00:00, 11.48s/it]\n"
     ]
    }
   ],
   "source": [
    "alpha_space = np.linspace(1.01, 1.15, 40)\n",
    "\n",
    "drop_cd = False  # True\n",
    "methods_list = metrics_df[\"Method\"].unique()\n",
    "\n",
    "current_res = calculate_metrics(\n",
    "    metrics_df, votenrank_list=[\"Minimax\", \"Copeland\"], drop_cd=drop_cd\n",
    ").reset_index()\n",
    "ranks_columns = [x for x in current_res.columns if \"value\" in x or \"cd_score\" == x]\n",
    "name2idx = {x: i for i, x in enumerate(ranks_columns)}\n",
    "res_sp_mean = {x: [] for x in ranks_columns}\n",
    "res_sp_std = {x: [] for x in ranks_columns}\n",
    "\n",
    "sampling = []\n",
    "\n",
    "\n",
    "best_res = {\"Value\": dict(metrics_df.groupby(by=\"Dataset\")[\"Value\"].max())}\n",
    "\n",
    "for alpha in tqdm(alpha_space):\n",
    "    tmp_sp_res = {x: [] for x in ranks_columns}\n",
    "    sampling.append(len(methods_list))\n",
    "    for method in methods_list:\n",
    "        new_method = method + \"*\"\n",
    "        new_df = {\n",
    "            \"Value\": dict(\n",
    "                metrics_df[metrics_df[\"Method\"] == method]\n",
    "                .groupby(by=\"Dataset\")[\"Value\"]\n",
    "                .max()\n",
    "            )\n",
    "        }\n",
    "        best_res_df = pd.DataFrame(new_df)\n",
    "        best_res_df.index.names = [\"Dataset\"]\n",
    "        best_res_df[\"Method\"] = new_method\n",
    "        incr_value = lambda x: increase_column(x, alpha)\n",
    "        best_res_df[\"Value\"] = best_res_df[\"Value\"].apply(incr_value)\n",
    "        best_res_df.reset_index(inplace=True)\n",
    "        tmp_df = pd.concat([metrics_df, best_res_df], ignore_index=True)\n",
    "\n",
    "        tmp_res = calculate_metrics(\n",
    "            tmp_df, votenrank_list=[\"Minimax\", \"Copeland\"], drop_cd=drop_cd\n",
    "        )\n",
    "        tmp_res.drop(index=new_method, inplace=True)\n",
    "\n",
    "        for r_method in ranks_columns:\n",
    "            tmp_sp_res[r_method].append(\n",
    "                stats.spearmanr(tmp_res[r_method], current_res[r_method])[0]\n",
    "            )\n",
    "    for r_method in ranks_columns:\n",
    "        res_sp_mean[r_method].append(np.mean(tmp_sp_res[r_method]))\n",
    "        res_sp_std[r_method].append(np.std(tmp_sp_res[r_method]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "\n",
    "plt.figure(figsize=(9, 5), dpi=200)\n",
    "for r_method in res_sp_mean:\n",
    "    if r_method != \"Minimax_value\":\n",
    "        plt.plot(\n",
    "            alpha_space,\n",
    "            res_sp_mean[r_method],\n",
    "            label=name_map[r_method],\n",
    "            # marker='.',\n",
    "            color=color_map[name2idx[r_method]],\n",
    "        )\n",
    "        # some confidence interval\n",
    "        ci = 1.96 * np.array(res_sp_std[r_method]) / np.sqrt(np.array(sampling))\n",
    "        plt.fill_between(\n",
    "            alpha_space,\n",
    "            np.array(res_sp_mean[r_method]) - ci,\n",
    "            np.array(res_sp_mean[r_method]) + ci,\n",
    "            color=color_map[name2idx[r_method]],\n",
    "            alpha=0.1,\n",
    "        )\n",
    "\n",
    "plt.legend(\n",
    "    loc=\"lower left\",\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "plt.xticks(\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "plt.yticks(\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "plt.xlabel(\n",
    "    r\"$\\alpha$\",\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "plt.ylabel(\n",
    "    r\"Spearman's $\\rho$\",\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "plt.show()\n",
    "plt.savefig(os.path.join(PATH_TO_RES, \"test_sp_add_sim_method_ci.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [15:15<00:00, 11.45s/it]\n"
     ]
    }
   ],
   "source": [
    "alpha_space = np.linspace(0.85, 1.15, 80)\n",
    "\n",
    "drop_cd = False\n",
    "methods_list = metrics_df[\"Method\"].unique()\n",
    "\n",
    "current_res = calculate_metrics(\n",
    "    metrics_df, votenrank_list=[\"Minimax\", \"Copeland\"], drop_cd=drop_cd\n",
    ").reset_index()\n",
    "ranks_columns = [x for x in current_res.columns if \"value\" in x or \"cd_score\" == x]\n",
    "name2idx = {x: i for i, x in enumerate(ranks_columns)}\n",
    "res_sp_mean = {x: [] for x in ranks_columns}\n",
    "res_sp_std = {x: [] for x in ranks_columns}\n",
    "\n",
    "sampling = []\n",
    "\n",
    "\n",
    "best_res = {\"Value\": dict(metrics_df.groupby(by=\"Dataset\")[\"Value\"].max())}\n",
    "\n",
    "for alpha in tqdm(alpha_space):\n",
    "    if alpha == 1.0:\n",
    "        continue\n",
    "    tmp_sp_res = {x: [] for x in ranks_columns}\n",
    "    sampling.append(len(methods_list))\n",
    "    for method in methods_list:\n",
    "        new_method = method + \"*\"\n",
    "        new_df = {\n",
    "            \"Value\": dict(\n",
    "                metrics_df[metrics_df[\"Method\"] == method]\n",
    "                .groupby(by=\"Dataset\")[\"Value\"]\n",
    "                .max()\n",
    "            )\n",
    "        }\n",
    "        best_res_df = pd.DataFrame(new_df)\n",
    "        best_res_df.index.names = [\"Dataset\"]\n",
    "        best_res_df[\"Method\"] = new_method\n",
    "        incr_value = lambda x: increase_column(x, alpha)\n",
    "        best_res_df[\"Value\"] = best_res_df[\"Value\"].apply(incr_value)\n",
    "        best_res_df.reset_index(inplace=True)\n",
    "        tmp_df = pd.concat([metrics_df, best_res_df], ignore_index=True)\n",
    "\n",
    "        tmp_res = calculate_metrics(\n",
    "            tmp_df, votenrank_list=[\"Minimax\", \"Copeland\"], drop_cd=drop_cd\n",
    "        )\n",
    "        tmp_res.drop(index=new_method, inplace=True)\n",
    "\n",
    "        for r_method in ranks_columns:\n",
    "            tmp_sp_res[r_method].append(\n",
    "                stats.spearmanr(tmp_res[r_method], current_res[r_method])[0]\n",
    "            )\n",
    "    for r_method in ranks_columns:\n",
    "        res_sp_mean[r_method].append(np.mean(tmp_sp_res[r_method]))\n",
    "        res_sp_std[r_method].append(np.std(tmp_sp_res[r_method]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "\n",
    "plt.figure(figsize=(9, 5), dpi=200)\n",
    "for r_method in res_sp_mean:\n",
    "    if r_method != \"Minimax_value\":\n",
    "        plt.plot(\n",
    "            alpha_space,\n",
    "            res_sp_mean[r_method],\n",
    "            label=name_map[r_method],\n",
    "            # marker='.',\n",
    "            color=color_map[name2idx[r_method]],\n",
    "        )\n",
    "        # some confidence interval\n",
    "        ci = 1.96 * np.array(res_sp_std[r_method]) / np.sqrt(np.array(sampling))\n",
    "        plt.fill_between(\n",
    "            alpha_space,\n",
    "            np.array(res_sp_mean[r_method]) - ci,\n",
    "            np.array(res_sp_mean[r_method]) + ci,\n",
    "            color=color_map[name2idx[r_method]],\n",
    "            alpha=0.1,\n",
    "        )\n",
    "\n",
    "plt.legend(\n",
    "    loc=\"lower left\",\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "plt.xticks(\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "plt.yticks(\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "plt.xlabel(\n",
    "    r\"$\\alpha$\",\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "plt.ylabel(\n",
    "    r\"Spearman's $\\rho$\",\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "plt.show()\n",
    "plt.savefig(os.path.join(PATH_TO_RES, \"test_sp_add_sim_method_ci_test.pdf\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
