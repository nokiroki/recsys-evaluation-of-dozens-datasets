{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "dir2 = os.path.abspath(\"\")\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path:\n",
    "    sys.path.append(dir1)\n",
    "os.chdir(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from hydra import compose, initialize\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "from scipy.stats import kurtosis, skew\n",
    "from src.preprocessing import ClassicDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from adjustText import adjust_text\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and file names\n",
    "config_path = os.path.join(\"..\", \"..\", \"config\", \"dataset\")\n",
    "config_files = [\n",
    "    os.path.splitext(f)[0]\n",
    "    for f in os.listdir(os.path.join(os.path.abspath(\".\"), config_path[6:]))\n",
    "    if f.endswith(\".yaml\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_features(\n",
    "    ratings: pd.DataFrame,\n",
    "    user_id: str = \"userId\",\n",
    "    item_id: str = \"itemId\",\n",
    "    rating_col: str = \"rating\",\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Calculate various features from the ratings DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        ratings (pd.DataFrame): DataFrame containing user-item ratings.\n",
    "        user_id (str): Name of the column representing user IDs.\n",
    "        item_id (str): Name of the column representing item IDs.\n",
    "        rating_col (str): Name of the column representing ratings.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing calculated features.\n",
    "    \"\"\"\n",
    "    # 1-3 Basic features\n",
    "    U = ratings[user_id].nunique()\n",
    "    I = ratings[item_id].nunique()\n",
    "    R = ratings.shape[0]\n",
    "\n",
    "    # 4 SpaceSize\n",
    "    SpaceSize = U * I\n",
    "\n",
    "    # 5 Shape\n",
    "    Shape = U / I\n",
    "\n",
    "    # 6  Density\n",
    "    Density = R / SpaceSize\n",
    "\n",
    "    # 7, 8 Rating per user and Rating per item\n",
    "    Rpu = R / U\n",
    "    Rpi = R / I\n",
    "\n",
    "    # 9, 10 Ginii and Giniu\n",
    "    Ri_counts = ratings[item_id].value_counts().values[::-1]\n",
    "    Ru_counts = ratings[user_id].value_counts().values[::-1]\n",
    "    Ginii = 1 - 2 * np.sum((I + 1 - np.arange(1, I + 1)) * Ri_counts) / ((I + 1) * R)\n",
    "    Giniu = 1 - 2 * np.sum((U + 1 - np.arange(1, U + 1)) * Ru_counts) / ((U + 1) * R)\n",
    "\n",
    "    # 11-14 Polularity biases\n",
    "    avg_item_popularity = ratings.groupby(item_id)[user_id].nunique() / U\n",
    "    ratings = ratings.join(avg_item_popularity.rename(\"item_popularity\", inplace=False), on=item_id)\n",
    "    popularity_bias = ratings.groupby(user_id)[\"item_popularity\"].mean()\n",
    "    APB = popularity_bias.mean()\n",
    "    StPB = popularity_bias.std()\n",
    "    SkPB = skew(popularity_bias)\n",
    "    KuPB = kurtosis(popularity_bias)\n",
    "\n",
    "    # 15-18 Long tail items\n",
    "    item_popularity = ratings[item_id].value_counts()\n",
    "    item_popularity_cumsum = item_popularity.cumsum()\n",
    "    LT_index = np.argmax(item_popularity_cumsum / R > 0.8)\n",
    "    LT_items = set(item_popularity.iloc[LT_index:].index)\n",
    "    LT_items_percentage = ratings.groupby(user_id)[item_id].apply(\n",
    "        lambda x: len(set(x) & LT_items) / len(set(x)) if len(set(x)) > 0 else 0\n",
    "    )\n",
    "    LTavg = LT_items_percentage.mean()\n",
    "    LTstd = LT_items_percentage.std()\n",
    "    LTsk = skew(LT_items_percentage)\n",
    "    LTku = kurtosis(LT_items_percentage)\n",
    "\n",
    "    return {\n",
    "        \"Nu\": U,\n",
    "        \"Ni\": I,\n",
    "        \"Nr\": R,\n",
    "        \"SpaceSize\": SpaceSize,\n",
    "        \"Shape\": Shape,\n",
    "        \"Density\": Density,\n",
    "        \"Rpu\": Rpu,\n",
    "        \"Rpi\": Rpi,\n",
    "        \"Ginii\": Ginii,\n",
    "        \"Giniu\": Giniu,\n",
    "        \"APB\": APB,\n",
    "        \"StPB\": StPB,\n",
    "        \"SkPB\": SkPB,\n",
    "        \"KuPB\": KuPB,\n",
    "        \"LTavg\": LTavg,\n",
    "        \"LTstd\": LTstd,\n",
    "        \"LTsk\": LTsk,\n",
    "        \"LTku\": LTku,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cold_items(\n",
    "    train_set: pd.DataFrame, val_set: pd.DataFrame, test_set: pd.DataFrame, column: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the fraction of items that only appear in the validation set and the test set,\n",
    "    but not in the training set.\n",
    "\n",
    "    Parameters:\n",
    "        train_set (pandas.DataFrame): The training dataset.\n",
    "        val_set (pandas.DataFrame): The validation dataset.\n",
    "        test_set (pandas.DataFrame): The test dataset.\n",
    "        column (str): The name of the column containing the items.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two fractions:\n",
    "            - fraction_items_only_in_val_set (float): The fraction of items only in the validation set.\n",
    "            - fraction_items_only_in_test_set (float): The fraction of items only in the test set.\n",
    "    \"\"\"\n",
    "    items_train_set = set(train_set[column])\n",
    "    items_val_set = set(val_set[column])\n",
    "    items_test_set = set(test_set[column])\n",
    "    fraction_items_only_in_val_set = len(items_val_set - items_train_set) / len(\n",
    "        items_val_set\n",
    "    )\n",
    "    fraction_items_only_in_test_set = len(items_test_set - items_train_set) / len(\n",
    "        items_test_set\n",
    "    )\n",
    "    return fraction_items_only_in_val_set, fraction_items_only_in_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_raw = dict()\n",
    "statistics_filtered = dict()\n",
    "statistics_wo_cold_iu = dict()\n",
    "for config_file in tqdm(config_files, desc=\"Processing config files\"):\n",
    "    with initialize(config_path=config_path, version_base=None):\n",
    "        cfg_data = compose(config_name=config_file)\n",
    "    # init some constants\n",
    "    user_id = cfg_data[\"user_column\"]\n",
    "    item_id = cfg_data[\"item_column\"]\n",
    "    rating_col = cfg_data[\"rating_column\"]\n",
    "    date_col = cfg_data[\"date_column\"]\n",
    "    splitting_conf = cfg_data[\"splitting\"]\n",
    "    # train_size = splitting_conf[\"train_size\"]\n",
    "    # val_size = splitting_conf[\"val_size\"]\n",
    "    # test_size = splitting_conf[\"test_size\"]\n",
    "    train_size = .8\n",
    "    val_size = .1\n",
    "    test_size = .1\n",
    "\n",
    "\n",
    "    if train_size + val_size + test_size != 1:\n",
    "        raise ValueError(\"Expected total fraction equal to 1\")\n",
    "\n",
    "    # get raw data\n",
    "    ratings = pd.read_parquet(\n",
    "        os.path.join(cfg_data[\"data_src\"], cfg_data[\"ratings_file\"])\n",
    "    )\n",
    "\n",
    "    # get raw dataset features\n",
    "    statistics_raw[cfg_data[\"name\"]] = get_dataset_features(\n",
    "        ratings, user_id, item_id, rating_col\n",
    "    )\n",
    "\n",
    "    # get filtered dataset features\n",
    "    ds = ClassicDataset()\n",
    "    ds.prepare(cfg_data)\n",
    "    interactions_processed = ds.prepared_data\n",
    "    \n",
    "    # get filtered dataset features\n",
    "    statistics_filtered[cfg_data[\"name\"]] = get_dataset_features(\n",
    "        interactions_processed, user_id, item_id, rating_col\n",
    "    )\n",
    "    # Make train/val/test split without filtering\n",
    "    interactions_processed.sort_values(date_col, inplace=True)\n",
    "    date_at_val_percentile = interactions_processed[date_col].quantile(\n",
    "        1 - val_size - test_size\n",
    "    )\n",
    "    date_at_test_percentile = interactions_processed[date_col].quantile(1 - test_size)\n",
    "\n",
    "    train_set = interactions_processed[\n",
    "        interactions_processed[date_col] <= date_at_val_percentile\n",
    "    ]\n",
    "    val_set = interactions_processed[\n",
    "        (interactions_processed[date_col] > date_at_val_percentile)\n",
    "        & (interactions_processed[date_col] <= date_at_test_percentile)\n",
    "    ]\n",
    "    test_set = interactions_processed[\n",
    "        interactions_processed[date_col] > date_at_test_percentile\n",
    "    ]\n",
    "\n",
    "    # Get cold users\n",
    "    cold_users_val, cold_users_test = get_cold_items(\n",
    "        train_set, val_set, test_set, user_id\n",
    "    )\n",
    "    statistics_filtered[cfg_data[\"name\"]][\"ColdUsersVal\"] = cold_users_val\n",
    "    statistics_filtered[cfg_data[\"name\"]][\"ColdUsersTest\"] = cold_users_test\n",
    "    # Get cold items\n",
    "    cold_items_val, cold_items_test = get_cold_items(\n",
    "        train_set, val_set, test_set, item_id\n",
    "    )\n",
    "    statistics_filtered[cfg_data[\"name\"]][\"ColdItemsVal\"] = cold_items_val\n",
    "    statistics_filtered[cfg_data[\"name\"]][\"ColdItemsTest\"] = cold_items_test\n",
    "\n",
    "    # filtering Cold users and Items\n",
    "    train_users = set(train_set[user_id])\n",
    "    train_items = set(train_set[item_id])\n",
    "    val_set = val_set[\n",
    "        val_set[user_id].isin(train_users) & val_set[item_id].isin(train_items)\n",
    "    ]\n",
    "    test_set = test_set[\n",
    "        test_set[user_id].isin(train_users) & test_set[item_id].isin(train_items)\n",
    "    ]\n",
    "\n",
    "    # get splitted and filtered dataset features\n",
    "    data_splitted = pd.concat([train_set, val_set, test_set])\n",
    "    data_splitted.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    statistics_wo_cold_iu[cfg_data[\"name\"]] = get_dataset_features(\n",
    "        data_splitted, user_id, item_id, rating_col\n",
    "    )\n",
    "    \n",
    "    # some additional features\n",
    "    statistics_wo_cold_iu[cfg_data[\"name\"]][\"FracInteractionsVal\"] = len(val_set) / (\n",
    "        len(data_splitted)\n",
    "    )\n",
    "    statistics_wo_cold_iu[cfg_data[\"name\"]][\"FracInteractionsTest\"] = len(test_set) / (\n",
    "        len(data_splitted)\n",
    "    )\n",
    "    \n",
    "    statistics_wo_cold_iu[cfg_data[\"name\"]][\"FracPassiveUsersVal\"] = val_set[user_id].nunique() / (\n",
    "        data_splitted[user_id].nunique()\n",
    "    )\n",
    "    statistics_wo_cold_iu[cfg_data[\"name\"]][\"FracPassiveUsersTest\"] = test_set[user_id].nunique() / (\n",
    "        data_splitted[user_id].nunique()\n",
    "    )\n",
    "    \n",
    "    statistics_wo_cold_iu[cfg_data[\"name\"]][\"FracPassiveItemsVal\"] = val_set[item_id].nunique() / (\n",
    "        data_splitted[item_id].nunique()\n",
    "    )\n",
    "    statistics_wo_cold_iu[cfg_data[\"name\"]][\"FracPassiveItemsTest\"] = test_set[item_id].nunique() / (\n",
    "        data_splitted[item_id].nunique()\n",
    "    )\n",
    "    \n",
    "    # timestamps \n",
    "    # full data\n",
    "    if len(str(int(ratings[date_col][0]))) > 11:\n",
    "        unit = 'ns'\n",
    "    elif len(str(int(ratings[date_col][0]))) > 6:\n",
    "        unit = 's'\n",
    "    else:\n",
    "        unit = 'W'\n",
    "        \n",
    "    data_splitted[date_col] = pd.to_datetime(data_splitted[date_col], unit=unit)\n",
    "    statistics_wo_cold_iu[cfg_data[\"name\"]][\"MinDate\"] = data_splitted[date_col].min().to_pydatetime()\n",
    "    statistics_wo_cold_iu[cfg_data[\"name\"]][\"MaxDate\"] = data_splitted[date_col].max().to_pydatetime()\n",
    "\n",
    "    # validation Set\n",
    "    val_set[date_col] = pd.to_datetime(val_set[date_col], unit=unit)\n",
    "    statistics_wo_cold_iu[cfg_data[\"name\"]][\"ValDate\"] = val_set[date_col].min().to_pydatetime()\n",
    "    statistics_wo_cold_iu[cfg_data[\"name\"]][\"DeltaVal\"] = (val_set[date_col].max() - val_set[date_col].min()).days\n",
    "\n",
    "    # test Set\n",
    "    test_set[date_col] = pd.to_datetime(test_set[date_col], unit=unit)\n",
    "    statistics_wo_cold_iu[cfg_data[\"name\"]][\"TestDate\"] =  test_set[date_col].min().to_pydatetime()\n",
    "    statistics_wo_cold_iu[cfg_data[\"name\"]][\"DeltaTest\"] = (test_set[date_col].max() - test_set[date_col].min()).days\n",
    "\n",
    "with open(os.path.join(\"results\", \"datasets_statistics\", \"statistics_raw.pcl\"), \"wb\") as f:\n",
    "    pickle.dump(statistics_raw, f)\n",
    "with open(os.path.join(\"results\", \"datasets_statistics\", \"statistics_filtered.pcl\"), \"wb\") as f:\n",
    "    pickle.dump(statistics_filtered, f)\n",
    "with open(os.path.join(\"results\", \"datasets_statistics\", \"statistics_wo_cold_iu.pcl\"), \"wb\") as f:\n",
    "    pickle.dump(statistics_wo_cold_iu, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"results\", \"datasets_statistics\", \"statistics_raw.pcl\"), \"rb\") as f:\n",
    "    statistics_raw = pickle.load(f)\n",
    "with open(os.path.join(\"results\", \"datasets_statistics\", \"statistics_filtered.pcl\"), \"rb\") as f:\n",
    "    statistics_filtered = pickle.load(f)\n",
    "with open(os.path.join(\"results\", \"datasets_statistics\", \"statistics_wo_cold_iu.pcl\"), \"rb\") as f:\n",
    "    statistics_wo_cold_iu = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary to a pandas DataFrame\n",
    "statistics_raw_df = pd.DataFrame(statistics_raw).T\n",
    "statistics_filtered_df = pd.DataFrame(statistics_filtered).T\n",
    "statistics_wo_cold_iu_df = pd.DataFrame(statistics_wo_cold_iu).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_wo_cold_iu_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_wo_cold_iu_df.SpaceSize.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_wo_cold_iu_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_wo_cold_iu_df.Density.sort_values()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['FracPassiveUsersVal', 'FracPassiveItemsVal', 'FracPassiveUsersTest', 'FracPassiveItemsTest']\n",
    "\n",
    "for column in columns:\n",
    "    data = [statistics_wo_cold_iu[key][column] for key in statistics_wo_cold_iu]\n",
    "    sns.histplot(data, kde=True, color='skyblue', bins=10)\n",
    "    plt.xlabel('Values')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(column)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms for splitting \n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(8, 8))\n",
    "\n",
    "columns = ['ColdUsersVal', 'ColdUsersTest', 'ColdItemsVal', 'ColdItemsTest']\n",
    "for i, column in enumerate(columns):\n",
    "    row, col = i // 2, i % 2\n",
    "    statistics_filtered_df[column].hist(ax=axes[row, col], color='skyblue', alpha=0.7, bins=30)\n",
    "    axes[row, col].set_title(column)\n",
    "    axes[row, col].set_xlabel(\"Value\")\n",
    "    axes[row, col].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_wo_cold_iu_df[(statistics_wo_cold_iu_df.FracInteractionsTest < 0.02) | (statistics_wo_cold_iu_df.FracInteractionsVal < 0.01)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_filtered_df[(statistics_filtered_df.ColdUsersTest > 0.9) | (statistics_filtered_df.ColdItemsTest > 0.9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_wo_cold_iu_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = statistics_wo_cold_iu_df[['Nu', 'Ni', 'Nr', 'SpaceSize', 'Shape', 'Density', 'Rpu', 'Rpi',\n",
    "       'Ginii', 'Giniu', 'APB', 'StPB', 'SkPB', 'KuPB', 'LTavg', 'LTstd',\n",
    "       'LTsk', 'LTku', 'FracInteractionsVal', 'FracInteractionsTest',\n",
    "       'FracPassiveUsersVal', 'FracPassiveUsersTest', 'FracPassiveItemsVal',\n",
    "       'FracPassiveItemsTest']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA with 2 components\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "pca_result = pca.fit_transform(data)\n",
    "\n",
    "# Create a new DataFrame for the PCA results\n",
    "pca_df = pd.DataFrame(data=pca_result, columns=[\"PC1\", \"PC2\"], index=data.index)\n",
    "\n",
    "# Plot the scatter plot of the first two principal components\n",
    "plt.figure(figsize=(18, 12))\n",
    "plt.scatter(pca_df[\"PC1\"], pca_df[\"PC2\"], s=100)\n",
    "plt.title(\"PCA - Datasets Comparison\")\n",
    "plt.grid()\n",
    "\n",
    "# Add text labels with adjust_text to avoid overlapping\n",
    "texts = [plt.text(x, y, dataset, fontsize=12) for dataset, (x, y) in pca_df.iterrows()]\n",
    "adjust_text(texts, arrowprops=dict(arrowstyle=\"-\", color=\"black\", lw=0.5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_results = tsne.fit_transform(data)\n",
    "\n",
    "plt.figure(figsize=(18, 12))\n",
    "plt.scatter(tsne_results[:, 0], tsne_results[:, 1], alpha=0.8)\n",
    "\n",
    "# Add labels using dataset names\n",
    "texts = []\n",
    "for i, dataset in enumerate(data.index):\n",
    "    texts.append(plt.text(tsne_results[i, 0], tsne_results[i, 1], dataset))\n",
    "\n",
    "adjust_text(\n",
    "    texts,\n",
    "    arrowprops=dict(arrowstyle=\"-\", color=\"gray\"),\n",
    "    autoalign=\"xy\",\n",
    "    force_points=0.1,\n",
    "    force_text=0.1,\n",
    ")\n",
    "\n",
    "plt.title(\"t-SNE Visualization of Datasets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension reduction using UMAP\n",
    "umap_reducer = umap.UMAP(n_neighbors=5, min_dist=0.1, metric=\"euclidean\", random_state=42)\n",
    "umap_results = umap_reducer.fit_transform(data)\n",
    "\n",
    "# Clustering using KMeans\n",
    "n_clusters = 5  \n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "kmeans_labels = kmeans.fit_predict(umap_results)\n",
    "\n",
    "# Get the size of each cluster\n",
    "cluster_sizes = np.bincount(kmeans_labels)\n",
    "\n",
    "# Calculate the scaling factor for circle sizes\n",
    "max_circle_size = 200 \n",
    "scaling_factor = max_circle_size / np.max(cluster_sizes)\n",
    "\n",
    "# Plot UMAP with cluster colors and circle sizes\n",
    "plt.figure(figsize=(18, 12))\n",
    "for cluster_num in range(n_clusters):\n",
    "    cluster_points = umap_results[kmeans_labels == cluster_num]\n",
    "    cluster_size = cluster_sizes[cluster_num]\n",
    "    circle_size = cluster_size * scaling_factor\n",
    "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], s=circle_size, label=f\"Cluster {cluster_num + 1}\")\n",
    "\n",
    "# Add labels using dataset names\n",
    "texts = []\n",
    "for i, dataset in enumerate(data.index):\n",
    "    texts.append(plt.text(umap_results[i, 0], umap_results[i, 1], dataset))\n",
    "\n",
    "adjust_text(\n",
    "    texts,\n",
    "    arrowprops=dict(arrowstyle=\"-\", color=\"gray\"),\n",
    "    autoalign=\"xy\",\n",
    "    force_points=0.1,\n",
    "    force_text=0.1,\n",
    ")\n",
    "\n",
    "plt.title(\"UMAP Visualization of Datasets with Clusters\")\n",
    "plt.xlabel(\"UMAP Component 1\")\n",
    "plt.ylabel(\"UMAP Component 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "df_normalized = pd.DataFrame(\n",
    "    scaler.fit_transform(data), columns=data.columns, index=data.index\n",
    ")\n",
    "plt.figure(figsize=(24, 18))\n",
    "sns.heatmap(df_normalized, annot=True, cmap=\"YlGnBu\", vmin=0, vmax=1, linewidths=0.5)\n",
    "plt.title(\"Comparison of Datasets Features\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Datasets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 6))\n",
    "sns.boxplot(data=df_normalized, palette=\"Set3\")\n",
    "plt.title(\"Distribution of Feature Values across Datasets\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"Feature Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 6))\n",
    "sns.violinplot(data=df_normalized, inner=\"quart\")\n",
    "plt.title(\"Distribution of Feature Values across Datasets\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"Feature Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate figures for each feature's histogram\n",
    "for feature in data.columns:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(data[feature], bins=30)\n",
    "    plt.xlabel(\"Feature Values\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"Distribution of {feature} in Datasets\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wind_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
