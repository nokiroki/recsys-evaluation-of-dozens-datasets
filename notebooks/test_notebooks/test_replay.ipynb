{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "libgomp: Invalid value for environment variable OMP_NUM_THREADS\n",
      "/home/user/conda/envs/recsys/lib/python3.9/site-packages/numba/core/config.py:154: RuntimeWarning: environ NUMBA_NUM_THREADS defined but failed to parse ''\n",
      "  warnings.warn(\"environ %s defined but failed to parse '%s'\" %\n",
      "\n",
      "libgomp: Invalid value for environment variable OMP_NUM_THREADS\n"
     ]
    }
   ],
   "source": [
    "from math import floor\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from replay.data_preparator import DataPreparator, Indexer\n",
    "from replay.experiment import Experiment\n",
    "from replay.metrics import Coverage, HitRate, NDCG, MAP\n",
    "from replay.model_handler import save, load, save_indexer, load_indexer\n",
    "from replay.models import ALSWrap, ItemKNN, SLIM\n",
    "from replay.session_handler import get_spark_session, State \n",
    "from replay.splitters import UserSplitter\n",
    "from replay.utils import convert2spark, get_log_info\n",
    "from replay.filters import filter_by_min_count, filter_out_low_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/22 17:33:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/22 17:33:34 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "spark_memory = floor(psutil.virtual_memory().total / 1024**3 * 0.7)\n",
    "driver_memory = f\"{spark_memory}g\"\n",
    "\n",
    "shuffle_partitions = os.cpu_count() * 3\n",
    "user_home = os.environ[\"HOME\"]\n",
    "\n",
    "session = (\n",
    "    SparkSession.builder.config(\"spark.driver.memory\", driver_memory)\n",
    "        .config(\n",
    "            \"spark.driver.extraJavaOptions\",\n",
    "            \"-Dio.netty.tryReflectionSetAccessible=true\",\n",
    "        )\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(shuffle_partitions))\n",
    "        .config(\"spark.local.dir\", os.path.join(user_home, \"tmp\"))\n",
    "        .config(\"spark.driver.maxResultSize\", \"4g\")\n",
    "        .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "        .config(\"spark.driver.host\", \"localhost\")\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "        .config(\"spark.kryoserializer.buffer.max\", \"256m\")\n",
    "        # .config(\"spark.worker.cleanup.enabled\", \"true\")\n",
    "        # .config(\"spark.worker.cleanup.interval\", \"5\")\n",
    "        # .config(\"spark.worker.cleanup.appDataTtl\", \"5\")\n",
    "        .master(\"local[*]\")\n",
    "        .enableHiveSupport()\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f18296454c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = State(session).session\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.startTime', '1692714814522'),\n",
       " ('spark.driver.memory', '1402g'),\n",
       " ('spark.sql.shuffle.partitions', '768'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.host', 'localhost'),\n",
       " ('spark.driver.port', '37049'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.driver.bindAddress', '127.0.0.1'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-Dio.netty.tryReflectionSetAccessible=true'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/home/jovyan/n.belousov/sber-recsys/notebooks/test_notebooks/spark-warehouse'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.local.dir', '/home/jovyan/tmp'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.kryoserializer.buffer.max', '256m'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.id', 'local-1692714815233'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.sql.execution.arrow.pyspark.enabled', 'true'),\n",
       " ('spark.driver.maxResultSize', '4g')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_config_param(session, conf_name):\n",
    "    # get current spark session configuration:\n",
    "    conf = session.sparkContext.getConf().getAll()\n",
    "    print(conf)\n",
    "    # get num partitions\n",
    "    print(f'{conf_name}: {dict(conf)[conf_name]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.driver.memory', '1402g'), ('spark.sql.shuffle.partitions', '768'), ('spark.repl.local.jars', 'file:/home/jovyan/n.belousov/sber-recsys/notebooks/test_notebooks/jars/replay_2.12-0.1_spark_3.1.jar'), ('spark.jars', 'jars/replay_2.12-0.1_spark_3.1.jar'), ('spark.executor.id', 'driver'), ('spark.driver.host', 'localhost'), ('spark.app.startTime', '1692714639594'), ('spark.app.name', 'pyspark-shell'), ('spark.driver.bindAddress', '127.0.0.1'), ('spark.driver.extraJavaOptions', '-Dio.netty.tryReflectionSetAccessible=true'), ('spark.driver.port', '32877'), ('spark.sql.warehouse.dir', 'file:/home/jovyan/n.belousov/sber-recsys/notebooks/test_notebooks/spark-warehouse'), ('spark.sql.catalogImplementation', 'hive'), ('spark.rdd.compress', 'True'), ('spark.local.dir', '/home/jovyan/tmp'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.kryoserializer.buffer.max', '256m'), ('spark.submit.deployMode', 'client'), ('spark.app.id', 'local-1692714640375'), ('spark.ui.showConsoleProgress', 'true'), ('spark.sql.execution.arrow.pyspark.enabled', 'true'), ('spark.driver.maxResultSize', '4g')]\n",
      "spark.sql.shuffle.partitions: 768\n"
     ]
    }
   ],
   "source": [
    "print_config_param(spark, 'spark.sql.shuffle.partitions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../../data/amazon_cds/CDs_and_Vinyl.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>itemId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001393774</td>\n",
       "      <td>A171I27YBM4FL6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1461888000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001393774</td>\n",
       "      <td>A1H1DL4K669VQ9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1461888000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001393774</td>\n",
       "      <td>A23WIHT5886G36</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1461024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001393774</td>\n",
       "      <td>A3SZNOJP8OL26X</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1459296000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001393774</td>\n",
       "      <td>A3V5XBBT7OZG5G</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1456185600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0001393774</td>\n",
       "      <td>A3SNL7UJY7GWBI</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1455148800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0001393774</td>\n",
       "      <td>A3478QRKQDOPQ2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1448668800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0001393774</td>\n",
       "      <td>A3CP0CNKNFCYBZ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1437177600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0001393774</td>\n",
       "      <td>A3OIIDZ137NJOU</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1436572800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0001393774</td>\n",
       "      <td>A3GVAG32NMMYT4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1432598400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       userId          itemId  rating   timestamp\n",
       "0  0001393774  A171I27YBM4FL6     5.0  1461888000\n",
       "1  0001393774  A1H1DL4K669VQ9     5.0  1461888000\n",
       "2  0001393774  A23WIHT5886G36     5.0  1461024000\n",
       "3  0001393774  A3SZNOJP8OL26X     5.0  1459296000\n",
       "4  0001393774  A3V5XBBT7OZG5G     5.0  1456185600\n",
       "5  0001393774  A3SNL7UJY7GWBI     5.0  1455148800\n",
       "6  0001393774  A3478QRKQDOPQ2     5.0  1448668800\n",
       "7  0001393774  A3CP0CNKNFCYBZ     4.0  1437177600\n",
       "8  0001393774  A3OIIDZ137NJOU     5.0  1436572800\n",
       "9  0001393774  A3GVAG32NMMYT4     4.0  1432598400"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preporator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preparator = DataPreparator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01-Aug-23 13:41:53, replay, INFO: Columns with ids of users or items are present in mapping. The dataframe will be treated as an interactions log.\n"
     ]
    }
   ],
   "source": [
    "log = preparator.transform(columns_mapping={\n",
    "    'user_id': 'userId',\n",
    "    'item_id': 'itemId',\n",
    "    'relevance': 'rating',\n",
    "    'timestamp': 'timestamp'\n",
    "}, data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+---------+-------------------+\n",
      "|   user_id|       item_id|relevance|          timestamp|\n",
      "+----------+--------------+---------+-------------------+\n",
      "|0001393774|A171I27YBM4FL6|      5.0|2016-04-29 00:00:00|\n",
      "|0001393774|A1H1DL4K669VQ9|      5.0|2016-04-29 00:00:00|\n",
      "+----------+--------------+---------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'total lines: 4543369, total users: 434060, total items: 1944316'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_log_info(log, user_col='user_id', item_col='item_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'total lines: 4543369, total users: 434060, total items: 1944316'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log = filter_out_low_ratings(log, .2)\n",
    "get_log_info(log, user_col='user_id', item_col='item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01-Aug-23 13:42:11, replay, INFO: current threshold removes 0.11964293457123998% of data\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'total lines: 3999787, total users: 145522, total items: 1792030'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log = filter_by_min_count(log, num_entries=5, group_by='user_id')\n",
    "get_log_info(log, user_col='user_id', item_col='item_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = Indexer(user_col='user_id', item_col='item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "indexer.fit(users=log.select('user_id'), items=log.select('item_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "log_replay = indexer.transform(df=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+-------------------+\n",
      "|user_idx|item_idx|relevance|          timestamp|\n",
      "+--------+--------+---------+-------------------+\n",
      "|  108495|  687152|      5.0|2017-01-25 00:00:00|\n",
      "|  108495|   82697|      5.0|2017-01-20 00:00:00|\n",
      "|  108495|  781273|      3.0|2017-01-11 00:00:00|\n",
      "|  108495|  890670|      1.0|2014-08-28 00:00:00|\n",
      "|  108495|  106139|      3.0|2014-08-27 00:00:00|\n",
      "+--------+--------+---------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "log_replay.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "splitter = UserSplitter(\n",
    "    drop_cold_items=True,\n",
    "    drop_cold_users=True,\n",
    "    item_test_size=K,\n",
    "    user_test_size=500,\n",
    "    seed=SEED,\n",
    "    shuffle=True\n",
    ")\n",
    "train, test = splitter.split(log_replay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 122:=============================================>         (30 + 4) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19997763 2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(train.count(), test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.is_cached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_knn = ItemKNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "item_knn.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "recs = item_knn.predict(train, K, users=test.select('user_idx').distinct(), filter_seen_items=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------------------+\n",
      "|user_idx|item_idx|         relevance|\n",
      "+--------+--------+------------------+\n",
      "|   12853|      40|  13.4743145837735|\n",
      "|   12853|      89| 12.78011963595636|\n",
      "|   12853|     108| 8.871822092603605|\n",
      "|   12853|      20| 8.112028529202195|\n",
      "|   12853|      49|5.9643452591206785|\n",
      "+--------+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recs.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sber-recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
