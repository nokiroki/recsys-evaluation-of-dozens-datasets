{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RePlay Tutorial\n",
    "This notebook is designed to familiarize with the use of RePlay library, including:\n",
    "- creating SparkSession or passing your own session to RePlay\n",
    "- data preprocessing\n",
    "- dataset users and items re-indexing\n",
    "- data splitting\n",
    "- model training and inference\n",
    "- model optimization\n",
    "- model saving and loading\n",
    "- models comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T16:01:45.639135Z",
     "start_time": "2020-02-10T16:01:45.612577Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from optuna.exceptions import ExperimentalWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ExperimentalWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from replay.data_preparator import DataPreparator, Indexer\n",
    "from replay.metrics import Coverage, HitRate, NDCG, MAP\n",
    "from replay.experiment import Experiment\n",
    "from replay.model_handler import save, load, save_indexer, load_indexer\n",
    "from replay.models import ALSWrap, ItemKNN, SLIM\n",
    "from replay.session_handler import get_spark_session, State \n",
    "from replay.splitters import UserSplitter\n",
    "from replay.utils import convert2spark, get_log_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "SEED=1234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RePlay uses Spark as a backend, and thus `SparkSession` should be created before RePlay running. Depends on your needs, you can choose, what to do about `SparkSession`.\n",
    "\n",
    "- Option 1: use default RePlay `SparkSession`\n",
    "- You can pass you own session to RePlay. Class `State` stores current session. Here you also have two options: \n",
    "    - Option 2: call `get_spark_session` to use default RePlay `SparkSession` with the custom driver memory and number of partitions \n",
    "    - Option 3: create `SparkSession` from scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: use default RePlay's SparkSession\n",
    "It is the simplest and recommended way for the local execution mode. RePlay will get existing SparkSession or create the new one with default configuration.  Default session parameters are stated in `replay/utils/session_handler.py` file. The driver memory volume and number of partitions depends on available RAM and number of cores respectively.\n",
    "\n",
    "You could initiate default session creation explicitly, e.g. to preprocess spark DataFrames, get link to SparkUI and set logging level, but if you do not create it by yourself, the session will be created by RePlay anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/19 16:00:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "23/09/19 16:00:23 WARN DependencyUtils: Local jar /app/notebooks/test_notebooks/jars/replay_2.12-0.1_spark_3.1.jar does not exist, skipping.\n",
      "23/09/19 16:00:23 INFO SparkContext: Running Spark version 3.1.3\n",
      "23/09/19 16:00:23 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "23/09/19 16:00:23 INFO ResourceUtils: ==============================================================\n",
      "23/09/19 16:00:23 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "23/09/19 16:00:23 INFO ResourceUtils: ==============================================================\n",
      "23/09/19 16:00:23 INFO SparkContext: Submitted application: pyspark-shell\n",
      "23/09/19 16:00:23 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "23/09/19 16:00:23 INFO ResourceProfile: Limiting resource is cpu\n",
      "23/09/19 16:00:23 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "23/09/19 16:00:23 INFO SecurityManager: Changing view acls to: new_recsys\n",
      "23/09/19 16:00:23 INFO SecurityManager: Changing modify acls to: new_recsys\n",
      "23/09/19 16:00:23 INFO SecurityManager: Changing view acls groups to: \n",
      "23/09/19 16:00:23 INFO SecurityManager: Changing modify acls groups to: \n",
      "23/09/19 16:00:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(new_recsys); groups with view permissions: Set(); users  with modify permissions: Set(new_recsys); groups with modify permissions: Set()\n",
      "23/09/19 16:00:23 INFO Utils: Successfully started service 'sparkDriver' on port 37663.\n",
      "23/09/19 16:00:23 INFO SparkEnv: Registering MapOutputTracker\n",
      "23/09/19 16:00:23 INFO SparkEnv: Registering BlockManagerMaster\n",
      "23/09/19 16:00:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "23/09/19 16:00:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "23/09/19 16:00:23 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/09/19 16:00:23 INFO DiskBlockManager: Created local directory at /home/new_recsys/tmp/blockmgr-086aef56-516e-451e-b6a7-a542f2d3ba0d\n",
      "23/09/19 16:00:23 INFO MemoryStore: MemoryStore started with capacity 46.8 GiB\n",
      "23/09/19 16:00:23 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "23/09/19 16:00:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "23/09/19 16:00:24 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://localhost:4040\n",
      "23/09/19 16:00:24 ERROR SparkContext: Failed to add jars/replay_2.12-0.1_spark_3.1.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /app/notebooks/test_notebooks/jars/replay_2.12-0.1_spark_3.1.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1929)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:1983)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:501)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:501)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:501)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/19 16:00:24 INFO Executor: Starting executor ID driver on host dd1ca78a7797\n",
      "23/09/19 16:00:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36353.\n",
      "23/09/19 16:00:24 INFO NettyBlockTransferService: Server created on localhost:36353\n",
      "23/09/19 16:00:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "23/09/19 16:00:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 36353, None)\n",
      "23/09/19 16:00:24 INFO BlockManagerMasterEndpoint: Registering block manager localhost:36353 with 46.8 GiB RAM, BlockManagerId(driver, localhost, 36353, None)\n",
      "23/09/19 16:00:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 36353, None)\n",
      "23/09/19 16:00:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 36353, None)\n",
      "23/09/19 16:00:24 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/app/notebooks/test_notebooks/spark-warehouse').\n",
      "23/09/19 16:00:24 INFO SharedState: Warehouse path is 'file:/app/notebooks/test_notebooks/spark-warehouse'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f49a4d7e5e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = State().session\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_config_param(session, conf_name):\n",
    "    # get current spark session configuration:\n",
    "    conf = session.sparkContext.getConf().getAll()\n",
    "    # get num partitions\n",
    "    print(f'{conf_name}: {dict(conf)[conf_name]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.sql.shuffle.partitions: 168\n"
     ]
    }
   ],
   "source": [
    "print_config_param(spark, 'spark.sql.shuffle.partitions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2:  Call `get_spark_session`  function to customize driver memory (spark.driver.memory) or number of partitions (spark.sql.shuffle.partitions) and use the default RePlay settings for other configuration parameters.\n",
    "We will specify 16 partitions and 4Gb driver memory for example. Pass created session to RePlay `State`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "session = get_spark_session(spark_memory=4, shuffle_partitions=16)\n",
    "spark = State(session).session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.sql.shuffle.partitions: 16\n"
     ]
    }
   ],
   "source": [
    "print_config_param(spark, 'spark.sql.shuffle.partitions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: Create your own session\n",
    "Pass created session to RePlay `State`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.sql.shuffle.partitions: 50\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "session = (\n",
    "        SparkSession.builder.config(\"spark.driver.memory\", \"8g\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"50\")\n",
    "        .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "        .config(\"spark.driver.host\", \"localhost\")\n",
    "        .master(\"local[*]\")\n",
    "        .enableHiveSupport()\n",
    "        .getOrCreate()\n",
    "    )\n",
    "spark = State(session).session\n",
    "print_config_param(spark, 'spark.sql.shuffle.partitions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Will return to the default session config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1321fe4c0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.stop()\n",
    "spark = State(get_spark_session()).session\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data preprocessing <a name='data-preparator'></a>\n",
    "We will use MovieLens 1m as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T15:59:42.041251Z",
     "start_time": "2020-02-10T15:59:09.230636Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/ml1m_ratings.dat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/app/notebooks/test_notebooks/01_replay_basics.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e65772d746573742d726563737973222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3139352e3133332e3231362e323039227d7d/app/notebooks/test_notebooks/01_replay_basics.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39mdata/ml1m_ratings.dat\u001b[39;49m\u001b[39m\"\u001b[39;49m, sep\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\"\u001b[39;49m, names\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39muserId\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mitem_id\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mrelevance\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtimestamp\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e65772d746573742d726563737973222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3139352e3133332e3231362e323039227d7d/app/notebooks/test_notebooks/01_replay_basics.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m users \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39mdata/ml1m_users.dat\u001b[39m\u001b[39m\"\u001b[39m, sep\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m, names\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgender\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mage\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39moccupation\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mzip_code\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers.py:688\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    635\u001b[0m     engine_specified \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    637\u001b[0m kwds\u001b[39m.\u001b[39mupdate(\n\u001b[1;32m    638\u001b[0m     delimiter\u001b[39m=\u001b[39mdelimiter,\n\u001b[1;32m    639\u001b[0m     engine\u001b[39m=\u001b[39mengine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    685\u001b[0m     skip_blank_lines\u001b[39m=\u001b[39mskip_blank_lines,\n\u001b[1;32m    686\u001b[0m )\n\u001b[0;32m--> 688\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers.py:454\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    451\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    453\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 454\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(fp_or_buf, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    456\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    457\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers.py:948\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwds:\n\u001b[1;32m    946\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m--> 948\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers.py:1180\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_make_engine\u001b[39m(\u001b[39mself\u001b[39m, engine\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mc\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   1179\u001b[0m     \u001b[39mif\u001b[39;00m engine \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mc\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 1180\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m CParserWrapper(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions)\n\u001b[1;32m   1181\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1182\u001b[0m         \u001b[39mif\u001b[39;00m engine \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpython\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers.py:2010\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39musecols, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39musecols_dtype \u001b[39m=\u001b[39m _validate_usecols_arg(kwds[\u001b[39m\"\u001b[39m\u001b[39musecols\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m   2008\u001b[0m kwds[\u001b[39m\"\u001b[39m\u001b[39musecols\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39musecols\n\u001b[0;32m-> 2010\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader \u001b[39m=\u001b[39m parsers\u001b[39m.\u001b[39;49mTextReader(src, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m   2011\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munnamed_cols \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader\u001b[39m.\u001b[39munnamed_cols\n\u001b[1;32m   2013\u001b[0m passed_names \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnames \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/user/conda/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:382\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/home/user/conda/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:674\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/ml1m_ratings.dat'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/ml1m_ratings.dat\", sep=\"\\t\", names=[\"userId\", \"item_id\", \"relevance\", \"timestamp\"])\n",
    "users = pd.read_csv(\"data/ml1m_users.dat\", sep=\"\\t\", names=[\"user_id\", \"gender\", \"age\", \"occupation\", \"zip_code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../../data/foursquare/interactions.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>venueCategoryId</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>470</td>\n",
       "      <td>4bf58dd8d48988d127951735</td>\n",
       "      <td>1.333462e+09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>979</td>\n",
       "      <td>4bf58dd8d48988d1df941735</td>\n",
       "      <td>1.333462e+09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId           venueCategoryId     timestamp  relevant\n",
       "0     470  4bf58dd8d48988d127951735  1.333462e+09         1\n",
       "1     979  4bf58dd8d48988d1df941735  1.333462e+09         1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id gender  age  occupation zip_code\n",
       "0        1      F    1          10    48067\n",
       "1        2      M   56          16    70072"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1. DataPreparator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An inner data format in RePlay is a spark dataframe. \n",
    "\n",
    "Columns with users' and items' identifiers are required for interaction log. Original user and item identifiers should be named as `user_id` and `item_id`. Those identifiers in section [0.3. Indexing](#indexing) will be converted to integer identifiers, which will be named `user_idx`, `item_idx`. Optional columns for interaction matrix are ``relevance`` and interaction ``timestamp``.\n",
    "\n",
    "DataFrames with user or item features should have column `user_id` or `item_id` respectively.\n",
    "\n",
    "We implemented DataPreparator class to convert pandas dataframes to spark format and preprocess the data, including renaming/creation of required and optional interaction matrix columns, null check and dates parsing. It is an optional step, if you already have data in Spark DataFrame format, could rename the above mentioned columns, and confident in completeness and quality of the data, skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preparator = DataPreparator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interactions log preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19-Sep-23 16:01:50, replay, INFO: Columns with ids of users or items are present in mapping. The dataframe will be treated as an interactions log.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 170 ms, sys: 24.1 ms, total: 194 ms\n",
      "Wall time: 6.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "log = preparator.transform(columns_mapping={'user_id': 'userId',\n",
    "                                      'item_id': 'venueCategoryId',\n",
    "                                      'relevance': 'relevant',\n",
    "                                      'timestamp': 'timestamp'\n",
    "                                     }, \n",
    "                           data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------------------+---------+\n",
      "|user_id|             item_id|          timestamp|relevance|\n",
      "+-------+--------------------+-------------------+---------+\n",
      "|    470|4bf58dd8d48988d12...|2012-04-03 18:00:09|      1.0|\n",
      "|    979|4bf58dd8d48988d1d...|2012-04-03 18:00:25|      1.0|\n",
      "+-------+--------------------+-------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- item_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- relevance: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'total lines: 227428, total users: 1083, total items: 400'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_log_info(log, user_col='user_id', item_col='item_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, `userId` was renamed to `user_id` and `timestamp` was converted to `TimestampType`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature dataframe preprocessing\n",
    "To transform feature dataframes you could also use DataPreparator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'users' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/app/notebooks/test_notebooks/01_replay_basics.ipynb Cell 35\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e65772d746573742d726563737973222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3139352e3133332e3231362e323039227d7d/app/notebooks/test_notebooks/01_replay_basics.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m user_features \u001b[39m=\u001b[39m preparator\u001b[39m.\u001b[39mtransform(columns_mapping\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m'\u001b[39m},\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e65772d746573742d726563737973222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3139352e3133332e3231362e323039227d7d/app/notebooks/test_notebooks/01_replay_basics.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m                                      data\u001b[39m=\u001b[39musers)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e65772d746573742d726563737973222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3139352e3133332e3231362e323039227d7d/app/notebooks/test_notebooks/01_replay_basics.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m user_features\u001b[39m.\u001b[39mshow(\u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'users' is not defined"
     ]
    }
   ],
   "source": [
    "user_features = preparator.transform(columns_mapping={'user_id': 'user_id'},\n",
    "                                     data=users)\n",
    "user_features.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the DataPreparator is optional, you could convert dataFrame to spark with ``convert_to_spark`` from ``replay.utils.spark_utils.py`` and manually rename columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'users' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/app/notebooks/test_notebooks/01_replay_basics.ipynb Cell 37\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e65772d746573742d726563737973222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3139352e3133332e3231362e323039227d7d/app/notebooks/test_notebooks/01_replay_basics.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# the same result without DataPreparator\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e65772d746573742d726563737973222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3139352e3133332e3231362e323039227d7d/app/notebooks/test_notebooks/01_replay_basics.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m convert2spark(users)\u001b[39m.\u001b[39mshow(\u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'users' is not defined"
     ]
    }
   ],
   "source": [
    "# the same result without DataPreparator\n",
    "convert2spark(users).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Filtering\n",
    "It is common to filter interactions log by interaction date or rating value or remove items or users with small number of interactions. RePlay offers some filters presented in `replay.preprocessig.filters` module.\n",
    "We will leave ratings greater than or equal to 3 and remove users with 4 or fewer interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replay.filters import filter_by_min_count, filter_out_low_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'total lines: 0, total users: 0, total items: 0'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log = filter_out_low_ratings(log, value=5)\n",
    "get_log_info(log, user_col='user_id', item_col='item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/replay/filters.py:46\u001b[0m, in \u001b[0;36mfilter_by_min_count\u001b[0;34m(data_frame, num_entries, group_by)\u001b[0m\n\u001b[1;32m     44\u001b[0m data_frame \u001b[39m=\u001b[39m data_frame\u001b[39m.\u001b[39mjoin(remaining_entities, on\u001b[39m=\u001b[39mgroup_by, how\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minner\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m output_count \u001b[39m=\u001b[39m data_frame\u001b[39m.\u001b[39mcount()\n\u001b[0;32m---> 46\u001b[0m diff \u001b[39m=\u001b[39m (input_count \u001b[39m-\u001b[39;49m output_count) \u001b[39m/\u001b[39;49m input_count\n\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m diff \u001b[39m>\u001b[39m \u001b[39m0.5\u001b[39m:\n\u001b[1;32m     48\u001b[0m     logger_level \u001b[39m=\u001b[39m State()\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mwarning\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "log = filter_by_min_count(log, num_entries=5, group_by='user_id')\n",
    "get_log_info(log, user_col='user_id', item_col='item_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='indexing'></a>\n",
    "### 0.3. Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RePlay models require columns with users' and items' identifiers _(ids)_ to be named as `user_idx` and `item_idx`. Those _ids_ should be integers starting at zero without gaps. This is important for models that use sparse matrices and define the matrix size as the biggest seen user and item index. Storing _ids_ as integers also help to reduce memory usage compared to string _ids_.\n",
    "\n",
    "You should convert user and item _ids_ in interaction's log and feature dataframes. RaPlay offers Indexer class to perform the _ids_ conversion and convert them back after recommendations generation (predict). The Indexer will store label encoders for users and items and allow transforming ids for users and items, which come after the Indexer fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = Indexer(user_col='user_id', item_col='item_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take all available user and item ids from log and features and pass them to Indexer. The _ids_ could repeat, the indexes will be ordered by label frequencies, so the most frequent label gets index 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.2 ms, sys: 11.9 ms, total: 63.1 ms\n",
      "Wall time: 3.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "indexer.fit(users=log.select('user_id').unionByName(log.select('user_id').distinct()),\n",
    "            items=log.select('item_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------------------+---------+\n",
      "|user_idx|item_idx|          timestamp|relevance|\n",
      "+--------+--------+-------------------+---------+\n",
      "|     326|     157|2012-04-03 18:00:09|      1.0|\n",
      "|     403|      33|2012-04-03 18:00:25|      1.0|\n",
      "+--------+--------+-------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n",
      "CPU times: user 75.1 ms, sys: 2.68 ms, total: 77.7 ms\n",
      "Wall time: 2.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "log_replay = indexer.transform(df=log)\n",
    "log_replay.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---+----------+--------+\n",
      "|user_idx|gender|age|occupation|zip_code|\n",
      "+--------+------+---+----------+--------+\n",
      "|    3861|     F|  1|        10|   48067|\n",
      "|    2301|     M| 56|        16|   70072|\n",
      "+--------+------+---+----------+--------+\n",
      "only showing top 2 rows\n",
      "\n",
      "CPU times: user 36.9 ms, sys: 7.24 ms, total: 44.1 ms\n",
      "Wall time: 352 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "user_features_replay = indexer.transform(df=user_features)\n",
    "user_features_replay.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4. Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RePlay provides you with data splitters to reproduce a validation schemas widely-used in recommender systems. Splitters return cached dataframes to compute them once and re-use for models training, inference and metrics calculation.\n",
    "\n",
    "`UserSplitter` takes ``item_test_size`` items for ``user_test_size`` user to the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T15:59:50.986401Z",
     "start_time": "2020-02-10T15:59:42.042998Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224928 2500\n",
      "CPU times: user 46.1 ms, sys: 10.8 ms, total: 56.9 ms\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "splitter = UserSplitter(\n",
    "    drop_cold_items=True,\n",
    "    drop_cold_users=True,\n",
    "    item_test_size=K,\n",
    "    user_test_size=500,\n",
    "    seed=SEED,\n",
    "    shuffle=True\n",
    ")\n",
    "train, test = splitter.split(log_replay)\n",
    "print(train.count(), test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.is_cached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Models training\n",
    "\n",
    "#### SLIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "slim = ItemKNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 101:==============================================>     (149 + 17) / 168]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 64.2 ms, sys: 15.9 ms, total: 80.1 ms\n",
      "Wall time: 11.2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "slim.fit(log=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.6 ms, sys: 24.3 ms, total: 76.9 ms\n",
      "Wall time: 9.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "recs = slim.predict(\n",
    "    k=K,\n",
    "    users=test.select('user_idx').distinct(),\n",
    "    log=train,\n",
    "    filter_seen_items=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------------------+\n",
      "|user_idx|item_idx|         relevance|\n",
      "+--------+--------+------------------+\n",
      "|     282|      10|1446.9782879185582|\n",
      "|     282|      14| 690.9596616351608|\n",
      "+--------+--------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recs.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Models evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RePlay implements some popular recommenders' quality metrics. Use pure metrics or calculate a set of chosen metrics and compare models with the ``Experiment`` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T16:07:28.942205Z",
     "start_time": "2020-02-10T16:07:26.281475Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "metrics = Experiment(test, {NDCG(): K,\n",
    "                            MAP() : K,\n",
    "                            HitRate(): [1, K],\n",
    "                            Coverage(train): K\n",
    "                           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 243:=========================================>          (135 + 16) / 168]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 209 ms, sys: 81.6 ms, total: 291 ms\n",
      "Wall time: 15.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coverage@5</th>\n",
       "      <th>HitRate@1</th>\n",
       "      <th>HitRate@5</th>\n",
       "      <th>MAP@5</th>\n",
       "      <th>NDCG@5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SLIM</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.00304</td>\n",
       "      <td>0.007292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Coverage@5  HitRate@1  HitRate@5    MAP@5    NDCG@5\n",
       "SLIM        0.18      0.004      0.032  0.00304  0.007292"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "metrics.add_result(\"SLIM\", recs)\n",
    "metrics.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameters optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split for hyperparameters optimization\n",
    "train_opt, val_opt = splitter.split(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-19 16:03:56,894] A new study created in memory with name: no-name-79bd8d9d-6520-4f66-b223-52ef7d5008de\n",
      "[I 2023-09-19 16:04:18,088] Trial 0 finished with value: 0.0036188326195130452 and parameters: {'num_neighbours': 10, 'shrink': 0, 'weighting': None}. Best is trial 0 with value: 0.0036188326195130452.\n",
      "[I 2023-09-19 16:04:34,883] Trial 1 finished with value: 0.005302278257183729 and parameters: {'num_neighbours': 36, 'shrink': 63, 'weighting': None}. Best is trial 1 with value: 0.005302278257183729.\n",
      "[I 2023-09-19 16:04:50,608] Trial 2 finished with value: 0.0040944696867979006 and parameters: {'num_neighbours': 14, 'shrink': 58, 'weighting': 'bm25'}. Best is trial 1 with value: 0.005302278257183729.\n",
      "[I 2023-09-19 16:05:06,264] Trial 3 finished with value: 0.005604317493639339 and parameters: {'num_neighbours': 41, 'shrink': 4, 'weighting': 'bm25'}. Best is trial 3 with value: 0.005604317493639339.\n",
      "[I 2023-09-19 16:05:21,973] Trial 4 finished with value: 0.004911426696532679 and parameters: {'num_neighbours': 31, 'shrink': 18, 'weighting': None}. Best is trial 3 with value: 0.005604317493639339.\n",
      "[I 2023-09-19 16:05:38,226] Trial 5 finished with value: 0.0058418577052159375 and parameters: {'num_neighbours': 61, 'shrink': 91, 'weighting': 'bm25'}. Best is trial 5 with value: 0.0058418577052159375.\n",
      "[I 2023-09-19 16:05:55,052] Trial 6 finished with value: 0.0061291378036887065 and parameters: {'num_neighbours': 93, 'shrink': 100, 'weighting': 'bm25'}. Best is trial 6 with value: 0.0061291378036887065.\n",
      "[I 2023-09-19 16:06:11,394] Trial 7 finished with value: 0.0061291378036887065 and parameters: {'num_neighbours': 89, 'shrink': 67, 'weighting': 'tf_idf'}. Best is trial 6 with value: 0.0061291378036887065.\n",
      "[I 2023-09-19 16:06:27,966] Trial 8 finished with value: 0.0054362470065820465 and parameters: {'num_neighbours': 27, 'shrink': 64, 'weighting': 'tf_idf'}. Best is trial 6 with value: 0.0061291378036887065.\n",
      "[I 2023-09-19 16:06:44,562] Trial 9 finished with value: 0.004386606386483313 and parameters: {'num_neighbours': 17, 'shrink': 32, 'weighting': 'tf_idf'}. Best is trial 6 with value: 0.0061291378036887065.\n",
      "[I 2023-09-19 16:06:59,382] Trial 10 finished with value: 0.0061291378036887065 and parameters: {'num_neighbours': 97, 'shrink': 97, 'weighting': 'bm25'}. Best is trial 6 with value: 0.0061291378036887065.\n",
      "[I 2023-09-19 16:07:14,972] Trial 11 finished with value: 0.0061291378036887065 and parameters: {'num_neighbours': 93, 'shrink': 82, 'weighting': 'tf_idf'}. Best is trial 6 with value: 0.0061291378036887065.\n",
      "[I 2023-09-19 16:07:30,153] Trial 12 finished with value: 0.006163353639775331 and parameters: {'num_neighbours': 71, 'shrink': 76, 'weighting': 'tf_idf'}. Best is trial 12 with value: 0.006163353639775331.\n",
      "[I 2023-09-19 16:07:44,758] Trial 13 finished with value: 0.006163353639775331 and parameters: {'num_neighbours': 71, 'shrink': 81, 'weighting': 'tf_idf'}. Best is trial 12 with value: 0.006163353639775331.\n",
      "[I 2023-09-19 16:08:00,427] Trial 14 finished with value: 0.006163353639775331 and parameters: {'num_neighbours': 68, 'shrink': 76, 'weighting': 'tf_idf'}. Best is trial 12 with value: 0.006163353639775331.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.63 s, sys: 773 ms, total: 3.41 s\n",
      "Wall time: 4min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_params = slim.optimize(train_opt, val_opt, criterion=NDCG(), k=K, budget=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_neighbours': 71, 'shrink': 76, 'weighting': 'tf_idf'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Compare with previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict_evaluate(model, experiment, name):\n",
    "    model.fit(log=train)\n",
    "\n",
    "    recs = model.predict(\n",
    "        k=K,\n",
    "        users=test.select('user_idx').distinct(),\n",
    "        log=train,\n",
    "        filter_seen_items=True\n",
    "    )\n",
    "\n",
    "    experiment.add_result(name, recs)\n",
    "    return recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 347 ms, sys: 124 ms, total: 472 ms\n",
      "Wall time: 35.2 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coverage@5</th>\n",
       "      <th>HitRate@1</th>\n",
       "      <th>HitRate@5</th>\n",
       "      <th>MAP@5</th>\n",
       "      <th>NDCG@5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SLIM_optimized</th>\n",
       "      <td>0.175</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.005287</td>\n",
       "      <td>0.012968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SLIM</th>\n",
       "      <td>0.180</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.007292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Coverage@5  HitRate@1  HitRate@5     MAP@5    NDCG@5\n",
       "SLIM_optimized       0.175      0.010      0.056  0.005287  0.012968\n",
       "SLIM                 0.180      0.004      0.032  0.003040  0.007292"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "recs = fit_predict_evaluate(ItemKNN(**best_params), metrics, 'SLIM_optimized')\n",
    "recs.cache() #caching for further processing\n",
    "metrics.results.sort_values('NDCG@5', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimized model was better on the validation dataset, but shows comparable quality on test (better by HitRate@5 and worse by the other quality metrics). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Getting final recommendations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return to original user and item identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2721:======================================>               (17 + 7) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------+\n",
      "|user_id|item_id|         relevance|\n",
      "+-------+-------+------------------+\n",
      "|   5107|    527| 1.046002020356746|\n",
      "|   5107|   2599|0.9492305434804991|\n",
      "+-------+-------+------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "CPU times: user 754 ms, sys: 296 ms, total: 1.05 s\n",
      "Wall time: 6.67 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "recs = indexer.inverse_transform(recs)\n",
    "recs.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to pandas or save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5107</td>\n",
       "      <td>527</td>\n",
       "      <td>1.046002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5107</td>\n",
       "      <td>2599</td>\n",
       "      <td>0.949231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  relevance\n",
       "0     5107      527   1.046002\n",
       "1     5107     2599   0.949231"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recs_pd = recs.toPandas()\n",
    "recs_pd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 2751:>                                                      (0 + 8) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.62 ms, sys: 4.24 ms, total: 8.86 ms\n",
      "Wall time: 2.75 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 2751:==================>                                    (8 + 8) / 24]\r\n",
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "recs.write.parquet(path='./slim_recs.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save and load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RePlay allows saving and loading fitted models with `save` and `load` functions of `model_handler` module. Model is saved as a folder with all necessary parameters and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 2762:>                                                       (0 + 1) / 1]\r\n",
      "\r\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 715 ms, sys: 273 ms, total: 988 ms\n",
      "Wall time: 4.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "save_indexer(indexer, './indexer_ml1')\n",
    "indexer = load_indexer('./indexer_ml1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 66.1 ms, sys: 59 ms, total: 125 ms\n",
      "Wall time: 42.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "save(slim, path='./slim_best_params')\n",
    "slim_loaded = load('./slim_best_params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.061429815496712774, 0.02613996164121192)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slim_loaded.beta, slim_loaded.lambda_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13-Jul-22 14:54:38, replay, INFO: This model can't predict cold items, they will be ignored\n",
      "[Stage 2842:========================================>             (18 + 6) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------------------+\n",
      "|user_idx|item_idx|         relevance|\n",
      "+--------+--------+------------------+\n",
      "|     282|      15|1.0362708717674016|\n",
      "|     282|      61|0.9560245729412186|\n",
      "+--------+--------+------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "CPU times: user 66.2 ms, sys: 33.3 ms, total: 99.5 ms\n",
      "Wall time: 10.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pred_from_loaded = slim_loaded.predict(k=K,\n",
    "    users=test.select('user_idx').distinct(),\n",
    "    log=train,\n",
    "    filter_seen_items=True)\n",
    "pred_from_loaded.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2868:==========================================>           (19 + 5) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------+\n",
      "|user_id|item_id|         relevance|\n",
      "+-------+-------+------------------+\n",
      "|   5107|    527|1.0362708717674016|\n",
      "|   5107|   2599|0.9560245729412186|\n",
      "+-------+-------+------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "CPU times: user 886 ms, sys: 341 ms, total: 1.23 s\n",
      "Wall time: 7.32 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "recs = indexer.inverse_transform(pred_from_loaded)\n",
    "recs.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Other RePlay models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ALS\n",
    "Commonly-used matrix factorization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13-Jul-22 14:55:47, replay, INFO: This model can't predict cold users, they will be ignored\n",
      "13-Jul-22 14:55:47, replay, INFO: This model can't predict cold items, they will be ignored\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 746 ms, sys: 450 ms, total: 1.2 s\n",
      "Wall time: 3min 11s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coverage@5</th>\n",
       "      <th>HitRate@1</th>\n",
       "      <th>HitRate@5</th>\n",
       "      <th>MAP@5</th>\n",
       "      <th>NDCG@5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SLIM</th>\n",
       "      <td>0.151916</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.100060</td>\n",
       "      <td>0.172416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SLIM_optimized</th>\n",
       "      <td>0.141163</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.099273</td>\n",
       "      <td>0.172007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALS</th>\n",
       "      <td>0.196305</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.086360</td>\n",
       "      <td>0.156045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Coverage@5  HitRate@1  HitRate@5     MAP@5    NDCG@5\n",
       "SLIM              0.151916      0.220      0.556  0.100060  0.172416\n",
       "SLIM_optimized    0.141163      0.214      0.568  0.099273  0.172007\n",
       "ALS               0.196305      0.202      0.550  0.086360  0.156045"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "recs = fit_predict_evaluate(ALSWrap(rank=100, seed=SEED), metrics, 'ALS')\n",
    "metrics.results.sort_values('NDCG@5', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ItemKNN\n",
    "Commonly-used item-based recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13-Jul-22 15:00:29, replay, INFO: This model can't predict cold items, they will be ignored\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 373 ms, sys: 413 ms, total: 786 ms\n",
      "Wall time: 3min 3s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coverage@5</th>\n",
       "      <th>HitRate@1</th>\n",
       "      <th>HitRate@5</th>\n",
       "      <th>MAP@5</th>\n",
       "      <th>NDCG@5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SLIM</th>\n",
       "      <td>0.151916</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.100060</td>\n",
       "      <td>0.172416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SLIM_optimized</th>\n",
       "      <td>0.141163</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.099273</td>\n",
       "      <td>0.172007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALS</th>\n",
       "      <td>0.196305</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.086360</td>\n",
       "      <td>0.156045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ItemKNN</th>\n",
       "      <td>0.050731</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.063387</td>\n",
       "      <td>0.113365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Coverage@5  HitRate@1  HitRate@5     MAP@5    NDCG@5\n",
       "SLIM              0.151916      0.220      0.556  0.100060  0.172416\n",
       "SLIM_optimized    0.141163      0.214      0.568  0.099273  0.172007\n",
       "ALS               0.196305      0.202      0.550  0.086360  0.156045\n",
       "ItemKNN               0.050731      0.168      0.390  0.063387  0.113365"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "recs = fit_predict_evaluate(ItemKNN(num_neighbours=100), metrics, 'ItemKNN')\n",
    "metrics.results.sort_values('NDCG@5', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Compare RePlay models with others\n",
    "To easily evaluate recommendations obtained from other sources, read and pass these recommendations to ``Experiment``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coverage@5</th>\n",
       "      <th>HitRate@1</th>\n",
       "      <th>HitRate@5</th>\n",
       "      <th>MAP@5</th>\n",
       "      <th>NDCG@5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SLIM</th>\n",
       "      <td>0.151916</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.100060</td>\n",
       "      <td>0.172416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SLIM_optimized</th>\n",
       "      <td>0.141163</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.099273</td>\n",
       "      <td>0.172007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALS</th>\n",
       "      <td>0.196305</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.086360</td>\n",
       "      <td>0.156045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ItemKNN</th>\n",
       "      <td>0.050731</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.063387</td>\n",
       "      <td>0.113365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my_model</th>\n",
       "      <td>0.050731</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.055720</td>\n",
       "      <td>0.104047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Coverage@5  HitRate@1  HitRate@5     MAP@5    NDCG@5\n",
       "SLIM              0.151916      0.220      0.556  0.100060  0.172416\n",
       "SLIM_optimized    0.141163      0.214      0.568  0.099273  0.172007\n",
       "ALS               0.196305      0.202      0.550  0.086360  0.156045\n",
       "ItemKNN               0.050731      0.168      0.390  0.063387  0.113365\n",
       "my_model          0.050731      0.118      0.390  0.055720  0.104047"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.add_result(\"my_model\", recs.withColumn(\"relevance\", sf.rand()))\n",
    "metrics.results.sort_values(\"NDCG@5\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "name": "movielens_nmf.ipynb",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "null"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
